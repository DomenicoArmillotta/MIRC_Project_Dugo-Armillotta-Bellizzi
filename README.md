# MIRC_Project:


## Da fare:
- riguardare e pulire il codice
- aggiungere una classe per la cache: crea un file con i termini del lexicon più cercati e li gestisce con politica LRU (la creazione e ricerca su file è analoga a quella del lexicon)
- vedere se vogliamo avere l'alternativa di usare tfidf e quindi aggiungere nel lexicon il term upper bound calcolato con tfidf
- togliere la cf appena siamo scuri al 101% che non la usiamo (aggiornare lunghezza entry del lexicon, vale anche per l'aggiunta della tub di tfidf)
- URGENTE: Testare con tutta la collezione (nell'ordine: creare docindex e parametri, eseguire spimi, calcolo maxscores e skipblocks); per fare ciò creiamo la pipeline con tutti gli step
- URGENTE pt2: Fare la documentazione e commentare per bene tutto il codice
- fare calcolo sulla distribuzione di tf e docid su l'indice non compresso e usarlo per giustificare la scelta sulla compressione
- fare trec eval. The qrels file:
  This file contains a list of documents considered relevants for each query. This relevance judgement is made by human beings who manually select documents that should be retrieved when a particular query is executed. This file can be considered as the "correct answer" and the documents retrieved by your IR system should approximate the maximum to it. It has the following format:


query-id 0 document-id relevance


The field query-id is a alphanumeric sequence to identify the query, document-id is a alphanumeric sequence to identify the judged document and the field relevance is a number to indicate the relevance degree between the document and query (0 for non relevant and 1 for relevant). The second field "0" is not currently used, just put it in the file. The fields can be separated by a blank space or tabulation.

The results file:
The results file contains a ranking of documents for each query automaticaly generated by your application. This is the file that will evaluated by trec_eval based in the "correct answer" provided by the first file. This file has the following format:


query-id Q0 document-id rank score STANDARD


The field query-id is a alphanumeric sequence to identify the query. The second field, with "Q0" value, is currently ignored by trec_eval, just put it in the file. The field document-id is a alphanumeric sequence to identify the retrieved document. The field rank is an integer value which represents the document position in the ranking, but this field is also ignored by trec_eval. The field score can be an integer or float value to indicate the similarity degree between document and query, so the most relevants docs will have higher scores. The last field, with "STANDARD" value, is used only to identify this run (this name is also showed in the output), you can use any alphanumeric sequence.